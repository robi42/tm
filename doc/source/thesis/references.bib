@book{Thomas2005,
  abstract = {Illuminating the Path is a call to action for researchers and developers to help safeguard our nation by transforming information overload into insights through visual analytics - the science of analytical reasoning facilitated by interactive visual interfaces. Achieving this will require interdisciplinary, collaborative efforts of researchers from throughout academia, industry, and the national laboratories.},
  archivePrefix = {arXiv},
  arxivId = {arXiv:1011.1669v3},
  author = {Thomas, James J and Cook, Kristin A},
  doi = {10.3389/fmicb.2011.00006},
  eprint = {arXiv:1011.1669v3},
  isbn = {0769523234},
  issn = {1664302X},
  journal = {IEEE Computer Society},
  publisher = {IEEE Computer Society},
  pages = {184},
  pmid = {21747779},
  title = {{Illuminating the Path: The Research and Development Agenda for Visual Analytics}},
  url = {http://vis.pnnl.gov/pdf/RD{\_}Agenda{\_}VisualAnalytics.pdf},
  year = {2005}
}

@book{Cooper2004,
  abstract = {Imagine, at a terrifyingly aggressive rate, everything you regularly use is being equipped with computer  technology. Think about your phone, cameras, cars - everything - being automated and programmed by people who in their   rush to accept the many benefits of the silicon chip, have abdicated their responsibility to make these products easy to  use. The Inmates are Running the Asylum argues that, despite appearances, business executives are simply not the ones in   control of the high-tech industry. They have inadvertently put programmers and engineers in charge, leading to products   and processes that waste money, squander customer loyalty, and erode competitive advantage. Business executives have let  the inmates run the asylum! In his book The Inmates Are Running the Asylum Alan Cooper calls for revolution - we need  technology to work in the same way average people think - we need to restore the sanity. He offers a provocative,  insightful and entertaining explanation of how talented people continuously design bad software-based products. More   importantly, he uses his own work with companies big and small to show how to harness those talents to create products  that will both thrill their users and grow the bottom line.},
  author = {Cooper, Alan},
  booktitle = {The Inmates Are Running the Asylum},
  edition = {2nd},
  doi = {10.1007/978-3-322-99786-9_1},
  isbn = {0-672-32614-0},
  issn = {10805699},
  pages = {288},
  pmid = {13443675},
  publisher = {Sams - Pearson Education},
  title = {{The Inmates Are Running the Asylum}},
  year = {2004}
}

@book{Norman2002,
  abstract = {Even the smartest among us can feel inept as we fail to figure out which switch turns on which light or stove burner, or whether to push, pull, or slide a door. The fault lies in product designs that ignore the needs of users and the principles of cognitive psychology. A bestseller in the United States, this classic work on the cognitive aspects of design contains examples of both good and bad design and simple rules that designers can use to improve the usability of objects as diverse as cars, computers, doors, and telephones.-From publisher description.},
  author = {Norman, Donald A},
  booktitle = {Human Factors and Ergonomics in Manufacturing},
  chapter = {7},
  doi = {10.1002/hfm.20127},
  isbn = {0465067107},
  issn = {03071766},
  pages = {272},
  pmid = {13182255},
  publisher = {Basic Books},
  title = {{The Design of Everyday Things}},
  url = {http://ucdwiki.chuank.com/uploads/Main/UCDReading\_wk5.pdf},
  year = {2002}
}

@book{Nielsen1993,
  abstract = {"The purpose of Jakob Nielsen's Usability Engineering is to help nontechnical people improve the systems so thatthey are not only error-free but also easier and more pleasant to use, and more efficient. It is a book that ...shows ushow to change the world and does so admirably....One of this book's strengths is that it provides a wide selection ofmethods for improving systems, and allows for the unavoidable constraints of the real world." -NEW SCIENTIST Written by the author of the best-selling HyperText {\&} HyperMedia, this book is an excellent guide to the methodsof usability engineering. The book provides the tools needed to avoid usability surprises and improve product quality.Step-by-step information on which method to use at various stages during the development lifecycle are included,along with detailed information on how to run a usability test and the unique issues relating to international usability. KEY FEATURESEmphasizes cost-effective methods that developers can implement immediately.Instructs readers about which methods to use and when to use them throughout the development lifecycle, ultimately helping in cost-benefit analysis. Shows readers how to avoid the four most frequently listed reasons for delay in software projects. Includes detailed information on how to run a usability test. Covers unique issues of international usability. Features an extensive bibliography allowing readers to find additional information. Written by an internationally renowned expert in the field and the author of the best-selling HyperText {\&} HyperMedia.},
  archivePrefix = {arXiv},
  arxivId = {arXiv:1011.1669v3},
  author = {Nielsen, Jakob},
  booktitle = {Usability Engineering},
  doi = {10.1145/1508044.1508050},
  eprint = {arXiv:1011.1669v3},
  isbn = {0125184069},
  issn = {10772626},
  pages = {362},
  pmid = {18369261},
  title = {{Usability Engineering}},
  url = {http://www.useit.com/jakob/useengbook.html},
  publisher = {Morgan Kaufmann},
  year = {1993}
}

@book{Manning2008,
  abstract = {Class-tested and coherent, this groundbreaking new textbook teaches web-era information retrieval,  including web search and the related areas of text classification and text clustering from basic concepts. Written   from a computer science perspective by three leading experts in the field, it gives an up-to-date treatment of all  aspects of the design and implementation of systems for gathering, indexing, and searching documents; methods for  evaluating systems; and an introduction to the use of machine learning methods on text collections. All the important  ideas are explained using examples and figures, making it perfect for introductory courses in information retrieval  for advanced undergraduates and graduate students in computer science. Based on feedback from extensive classroom  experience, the book has been carefully structured in order to make teaching more natural and effective. Although  originally designed as the primary text for a graduate or advanced undergraduate course in information retrieval, the  book will also create a buzz for researchers and professionals alike.},
  archivePrefix = {arXiv},
  arxivId = {0521865719 9780521865715},
  author = {Manning, Christopher D and Raghavan, Prabhakar and Sch\"{u}tze, Hinrich},
  booktitle = {Online},
  doi = {10.1109/LPT.2009.2020494},
  eprint = {0521865719 9780521865715},
  isbn = {0521865719},
  issn = {13864564},
  pages = {496},
  pmid = {10575050},
  publisher = {Cambridge University Press},
  title = {{Introduction to Information Retrieval}},
  url = {http://nlp.stanford.edu/IR-book/},
  edition = {1st},
  year = {2008}
}

@book{Aigner2011,
  title = {Visualization of Time-Oriented Data},
  series = {Human-Computer Interaction},
  year = {2011},
  pages = {286},
  publisher = {Springer},
  organization = {Springer},
  edition = {1st},
  location = {London, UK},
  abstract = {<p>Time is an exceptional dimension that is common to many application domains such as medicine, engineering, business, science, biography, history, planning, or project management. Understanding time-oriented data enables us to learn from the past in order to predict, plan, and build the future. Due to the distinct characteristics of time, appropriate visual and analytical methods are required to explore and analyze them.</p>
<div class="springerHTML">
  <p>This book starts with an introduction to visualization and a number of historical examples of visual representations. At its core, the book presents and discusses a systematic view of the visualization of time-oriented data. This view is structured along three key questions. While the aspects of time and associated data describe <i>what</i> is being visualized, user tasks are related to the question <i>why</i> something is visualized. These characteristics and tasks determine <i>how </i>the visualization is to be designed. To support visual exploration, interaction techniques and analytical methods are required as well, which are discussed in separate chapters. The concepts explained in this book are illustrated with numerous examples.</p>
  <p>A large part of this book is devoted to a structured survey of existing techniques for visualizing time and time-oriented data. Overall, 101 different visualization techniques are presented on a per-page basis; each of these self-contained descriptions is accompanied by an illustration and corresponding references.\&nbsp; This survey serves as a reference for scientists conducting related research as well as for practitioners seeking information on how their time-oriented data can best be visualized in order to gain valuable insights.</p>
</div>
},
  keywords = {Analysis, Computer-Generated Visual Representations, Human-computer Interaction, Time-Oriented Data, User, Visual analytics, Visualization},
  isbn = {978-0-85729-078-6},
  doi = {10.1007/978-0-85729-079-3},
  url = {http://www.timeviz.net},
  author = {Aigner, Wolfgang and Miksch, Silvia and Schumann, Heidrun and Tominski, Christian}
}

@book{Witten2011,
  abstract = {Like the popular second edition, Data Mining: Practical Machine Learning Tools and Techniques offers a thorough grounding in machine learning concepts as well as practical advice on applying machine learning tools and techniques in real-world data mining situations. Inside, you'll learn all you need to know about preparing inputs, interpreting outputs, evaluating results, and the algorithmic methods at the heart of successful data mining?including both tried-and-true techniques of today as well as methods at the leading edge of contemporary research. Complementing the book is a fully functional platform-independent open source Weka software for machine learning, available for free download. The book is a major revision of the second edition that appeared in 2005. While the basic core remains the same, it has been updated to reflect the changes that have taken place over the last four or five years. The highlights for the updated new edition include completely revised technique sections; new chapter on Data Transformations, new chapter on Ensemble Learning, new chapter on Massive Data Sets, a new ?book release? version of the popular Weka machine learning open source software (developed by the authors and specific to the Third Edition); new material on ?multi-instance learning?; new information on ranking the classification, plus comprehensive updates and modernization throughout. All in all, approximately 100 pages of new material. Thorough grounding in machine learning concepts as well as practical advice on applying the tools and techniques Algorithmic methods at the heart of successful data mining?including tired and true methods as well as leading edge methods Performance improvement techniques that work by transforming the input or output Downloadable Weka, a collection of machine learning algorithms for data mining tasks, including tools for data pre-processing, classification, regression, clustering, association rules, and visualization?in an updated, interactive interface.},
  author = {Witten, Ian H and Frank, Eibe and Hall, Mark A},
  isbn = {9780123748560},
  edition = {4th},
  pages = {654},
  publisher = {Morgan Kaufmann},
  series = {Morgan Kaufmann Series in Data Management Systems},
  title = {{Data Mining: Practical Machine Learning Tools and Techniques}},
  url = {http://www.amazon.com/Data-Mining-Practical-Techniques-Management/dp/0123748569},
  year = {2016}
}

@book{Dasu2003,
  author = {Dasu, Tamraparni and Johnson, Theodore},
  isbn = {9780471268512},
  publisher = {Wiley-Interscience},
  series = {Wiley Series in Probability and Statistics},
  title = {{Exploratory Data Mining and Data Cleaning}},
  url = {http://books.google.at/books?id=2OWJVkevamQC},
  year = {2003}
}

@book{Tufte2001,
  abstract = {I: Graphical practice Graphical excellence Graphical integrity Sources of graphical integrity and   sophistication II: ry of data graphics Data-ink and graphical redesign Chartjunk: vibrations, grids, and ducks Data-ink   maximization and graphical design Multifunctioning graphical elements Data density and small multiples Aesthetics and   technique in data graphical design Epilogue: Designs for the display of information.},
  archivePrefix = {arXiv},
  arxivId = {ISBN 0-9613921-4-2},
  author = {Tufte, Edward R},
  booktitle = {Technometrics},
  doi = {10.1198/tech.2002.s78},
  eprint = {ISBN 0-9613921-4-2},
  isbn = {0961392142},
  issn = {09613921},
  publisher = {Graphics Pr},
  pages = {197},
  pmid = {12462603},
  title = {{The Visual Display of Quantitative Information}},
  url = {http://www.amazon.co.uk/dp/0961392142},
  edition = {2nd},
  year = {2001}
}

@book{Newman2015,
  abstract = {Distributed systems have become more fine-grained in the past 10 years, shifting from code-heavy monolithic applications to smaller, self-contained microservices. But developing these systems brings its own set of headaches. With lots of examples and practical advice, this book takes a holistic view of the topics that system architects and administrators must consider when building, managing, and evolving microservice architectures.Microservice technologies are moving quickly. Author Sam Newman provides you with a firm grounding in the concepts while diving into current solutions for modeling, integrating, testing, deploying, and monitoring your own autonomous services. You'll follow a fictional company throughout the book to learn how building a microservice architecture affects a single domain.Discover how microservices allow you to align your system design with your organization's goalsLearn options for integrating a service with the rest of your systemTake an incremental approach when splitting monolithic codebasesDeploy individual microservices through continuous integrationExamine the complexities of testing and monitoring distributed servicesManage security with user-to-service and service-to-service modelsUnderstand the challenges of scaling microservice architectures},
  author = {Newman, Sam},
  publisher = {O'Reilly Media},
  isbn = {978-1-491-95035-7},
  keywords = {www.it-ebooks.info},
  pages = {280},
  title = {{Building Microservices}},
  url = {http://samnewman.io/books/building_microservices/},
  year = {2015}
}

@book{Gormley2015,
  author = {Gormley, Clinton and Tong, Zachary},
  title = {Elasticsearch: The Definitive Guide},
  year = {2015},
  isbn = {1449358543, 9781449358549},
  edition = {1st},
  publisher = {O'Reilly Media},
}

@book{Garrett2011,
  abstract = {From the moment it was published almost ten years ago, Elements of User Experience became a vital reference for web and interaction designers the world over, and has come to define the core principles of the practice. Now, in this updated, expanded, and full-color new edition, Jesse James Garrett has refined his thinking about the Web, going beyond the desktop to include information that also applies to the sudden proliferation of mobile devices and applications. Successful interaction design requires more than just creating clean code and sharp graphics. You must also fulfill your strategic objectives while meeting the needs of your users. Even the best content and the most sophisticated technology won't help you balance those goals without a cohesive, consistent user experience to support it. With so many issues involved—usability, brand identity, information architecture, interaction design— creating the user experience can be overwhelmingly complex. This new edition of The Elements of User Experience cuts through that complexity with clear explanations and vivid illustrations that focus on ideas rather than tools or techniques. Garrett gives readers the big picture of user experience development, from strategy and requirements to information architecture and visual design.},
  author = {Garrett, Jesse James},
  publisher = {New Riders},
  doi = {10.1145/889692.889709},
  isbn = {9780321683687},
  issn = {10725520},
  pages = {3--54},
  pmid = {18780720},
  title = {{The Elements of User Experience: User-Centered Design for the Web and Beyond}},
  edition = {2nd},
  year = {2011}
}

@book{Card1999,
  abstract = {This groundbreaking book defines the emerging field of information visualization and offers the first-ever collection of the classic papers of the discipline, with introductions and analytical discussions of each topic and paper. The authors' intention is to present papers that focus on the use of visualization to discover relationships, using interactive graphics to amplify thought. This book is intended for research professionals in academia and industry; new graduate students and professors who want to begin work in this burgeoning field; professionals involved in financial data analysis, statistics, and information design; scientific data managers; and professionals involved in medical, bioinformatics, and other areas. Full-color reproduction throughout Author power team - an exciting and timely collaboration between the field's pioneering, most-respected names The only book on Information Visualization with the depth necessary for use as a text or as a reference for the information professional Text includes the classic source papers as well as a collection of cutting edge work},
  author = {Card, Stuart K and Mackinlay, Jock D and Shneiderman, Ben},
  title = {Readings in Information Visualization: Using Vision to Think},
  year = {1999},
  isbn = {1-55860-533-9},
  publisher = {Morgan Kaufmann},
  address = {San Francisco, CA, USA},
}

@article{Miksch2014,
  author = {Miksch, Silvia and Aigner, Wolfgang},
  title = {{Special Section on Visual Analytics: A Matter of Time: Applying a Data--Users--Tasks Design Triangle to Visual  Analytics of Time-Oriented Data}},
  journal = {Computer \& Graphics},
  issue_date = {February, 2014},
  volume = {38},
  month = feb,
  year = {2014},
  issn = {0097-8493},
  pages = {286--290},
  numpages = {5},
  url = {http://dx.doi.org/10.1016/j.cag.2013.11.002},
  doi = {10.1016/j.cag.2013.11.002},
  acmid = {2577856},
  publisher = {Pergamon Press, Inc.},
  address = {Elmsford, NY, USA},
  keywords = {Interaction design, Interactive visualization, Temporal data mining, Time-oriented data, Visual analytics},
}

@article{Munzner2009,
  author = {Munzner, Tamara},
  title = {{A Nested Model for Visualization Design and Validation}},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  issue_date = {November 2009},
  volume = {15},
  number = {6},
  month = nov,
  year = {2009},
  issn = {1077-2626},
  pages = {921--928},
  numpages = {8},
  url = {http://dx.doi.org/10.1109/TVCG.2009.111},
  doi = {10.1109/TVCG.2009.111},
  publisher = {IEEE Educational Activities Department},
  address = {Piscataway, NJ, USA}
}

@article{Holzinger2005,
  author = {Holzinger, Andreas},
  title = {{Usability Engineering Methods for Software Developers}},
  journal = {Communications of the ACM},
  issue_date = {January 2005},
  volume = {48},
  number = {1},
  month = jan,
  year = {2005},
  issn = {0001-0782},
  pages = {71--74},
  numpages = {4},
  url = {http://doi.acm.org/10.1145/1039539.1039541},
  doi = {10.1145/1039539.1039541},
  acmid = {1039541},
  publisher = {ACM},
  address = {New York, NY, USA},
}

@article{Prokopec2010,
  abstract = {Most applications manipulate structured data. Modern languages and platforms provide collection frameworks with basic data structures like lists, hashtables and trees. These data structures come with a range of predefined operations which include sorting, filtering or finding elements. Such bulk operations usually traverse the entire collection and process the elements sequentially. Their implementation often relies on iterators, which are not applicable to parallel operations due to their sequential nature. We present an approach to parallelizing collection operations in a generic way, which can be used to factor out common parallel operations in collection libraries. Our framework is easy to use and straightforward to extend to new collections. We show how to implement concrete parallel collections such as parallel arrays and parallel hash maps, proposing an efficient solution to parallel hash map construction. Finally, we give benchmarks showing the performance of parallel collection operations.},
  author = {Prokopec, Aleksandar and Rompf, Tiark and Bagwell, Phil and Odersky, Martin},
  journal = {Builder},
  keywords = {parallel collections,parallel data structures,scala},
  pages = {1--10},
  title = {{A Generic Parallel Collection Framework}},
  url = {http://infoscience.epfl.ch/record/150220/files/pc.pdf?version=1},
  year = {2010}
}

@article{Raman2001a,
  abstract = {Cleaning data of errors in structure and content is important for data warehousing and integration. Current solutions for data cleaning involve many iterations of data auditing to find errors, and long-running transformations to fix them. Users need to endure long waits, and often write complex transformation scripts. We present Potters Wheel, an interactive data cleaning system that tightly integrates transformation and discrepancy detection. Users gradually build transformations to clean the data by adding or undoing transforms on a spreadsheet-like interface; the effect of a transform is shown at once on records visible on screen. These transforms are specified either through simple graphical operations, or by showing the desired effects on example data values. In the background, PottersWheel automatically infers structures for data values in terms of user-defined domains, and accordingly checks for constraint violations. Thus users can gradually build a transformation as discrepancies are found, and clean the data without writing complex programs or enduring long delays.},
  author = {Raman, Vijayshankar and Hellerstein, Joseph M},
  doi = {10.1.1.39.2790},
  editor = {Apers, Peter M G and Atzeni, Paolo and Ceri, Stefano and Paraboschi, Stefano and Ramamohanarao, Kotagiri and Snodgrass, Richard T},
  isbn = {1558608044},
  issn = {10477349},
  journal = {VLDB},
  pages = {381--390},
  publisher = {Morgan Kaufmann},
  title = {{Potter's Wheel: An Interactive Data Cleaning System}},
  url = {http://www.vldb.org/conf/2001/P381.pdf},
  volume = {01},
  year = {2001}
}

@inproceedings{Kandel2011a,
  abstract = {Though data analysis tools continue to improve, analysts still expend an inordinate amount of time and effort manipulating data and assessing data quality issues. Such "data wrangling" regularly involves reformatting data values or layout, correcting erroneous or missing values, and integrating multiple data sources. These transforms are often difficult to specify and difficult to reuse across analysis tasks, teams, and tools. In response, we introduce Wrangler, an interactive system for creating data transformations. Wrangler combines direct manipulation of visualized data with automatic inference of relevant transforms, enabling analysts to iteratively explore the space of applicable operations and preview their effects. Wrangler leverages semantic data types (e.g., geographic locations, dates, classification codes) to aid validation and type conversion. Interactive histories support review, refinement, and annotation of transformation scripts. User study results show that Wrangler significantly reduces specification time and promotes the use of robust, auditable transforms instead of manual editing.},
  author = {Kandel, Sean and Paepcke, Andreas and Hellerstein, Joseph and Heer, Jeffrey},
  title = {{Wrangler: Interactive Visual Specification of Data Transformation Scripts}},
  booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
  series = {CHI '11},
  year = {2011},
  isbn = {978-1-4503-0228-9},
  location = {Vancouver, BC, Canada},
  pages = {3363--3372},
  numpages = {10},
  url = {http://doi.acm.org/10.1145/1978942.1979444},
  doi = {10.1145/1978942.1979444},
  acmid = {1979444},
  publisher = {ACM},
  address = {New York, NY, USA},
  keywords = {data analysis, data cleaning, transformation, visualization, wrangler}
}

@article{Kandel2011b,
  abstract = {In spite of advances in technologies for working with data, analysts still spend an inordinate amount of time diagnosing data quality issues and manipulating data into a usable form. This process of data wrangling often constitutes the most tedious and time-consuming aspect of analysis. Though data cleaning and integration arelongstanding issues in the database community, relatively little research has explored how interactive visualization can advance the state of the art. In this article, we review the challenges and opportunities associated with addressing data quality issues. We argue that analysts might more effectively wrangle data through new interactive systems that integrate data verification, transformation, and visualization. We identify a number of outstanding research questions, including how appropriate visual encodings can facilitate apprehension of missing data, discrepant values, and uncertainty; how interactive visualizations might facilitate data transform specification; and how recorded provenance and social interaction might enable wider reuse, verification, and modification of data transformations.},
  author = {Kandel, Sean and Heer, Jeffrey and Plaisant, Catherine and Kennedy, Jessie and Ham, Frank Van and Riche, Nathalie Henry and Weaver, Chris and Lee, Bongshin and Brodbeck, Dominique and Buono, Paolo},
  journal = {Information Visualization},
  keywords = {qa75 electronic computers. computer science},
  number = {4},
  pages = {271--288},
  publisher = {SAGE Publications},
  title = {{Research Directions in Data Wrangling: Visualizations and Transformations for Usable and Credible Data}},
  url = {http://researchrepository.napier.ac.uk/4767/},
  volume = {10},
  month = Jan,
  year = {2011}
}

@article{Gulwani2010,
  abstract = {Program Synthesis, which is the task of discovering programs that realize user intent, can be useful in several scenarios: discovery of new algorithms, helping regular programmers automatically discover tricky/mundane programming details, enabling people with no programming background to develop scripts for performing repetitive tasks (end-user programming), and even problem solving in the context of automating teaching. In this tutorial, I will describe the three key dimensions that should be taken into account in designing any program synthesis system: expression of user intent, space of programs over which to search, and the search technique [1]. (i) The user intent can be expressed in the form of logical relations between inputs and outputs, input-output examples, demonstrations, natural language, and inefficient or related programs. (ii) The search space can be over imperative or functional programs (with possible restrictions on the control structure or the operator set), or over restricted models of computations such as regular/context-free grammars/transducers, or succinct logical representations. (iii) The search technique can be based on exhaustive search, version space algebras, machine learning techniques (such as belief propagation or genetic programming), or logical reasoning techniques based on SAT/SMT solvers. I will illustrate these concepts by brief description of various program synthesis projects that target synthesis of a wide variety of programs such as standard undergraduate textbook algorithms (e.g., sorting, dynamic programming), program inverses (e.g., decoders, deserializers), bitvector manipulation routines, deobfuscated programs, graph algorithms, text-manipulating routines, geometry algorithms etc.},
  author = {Gulwani, Sumit},
  doi = {10.1145/1836089.1836091},
  isbn = {978-1-4577-0734-6},
  journal = {Principles and Practice of Declarative Programming},
  keywords = {belief,deductive synthesis,genetic programming,inductive synthesis,machine learning,ming by examples,probabilistic inference,program-,programming by demonstration,propagation,sat solving,smt solving},
  pages = {1--1},
  title = {{Dimensions in Program Synthesis}},
  booktitle={{Formal Methods in Computer Aided Design}},
  publisher = {ACM},
  month = Oct,
  year = {2010}
}

@article{Gulwani2011,
  abstract = {We describe the design of a string programming/expression language that supports restricted forms of regular expressions, conditionals and loops. The language is expressive enough to represent a wide variety of string manipulation tasks that end-users struggle with. We describe an algorithm based on several novel concepts for synthesizing a desired program in this language from input-output examples. The synthesis algorithm is very efficient taking a fraction of a second for various benchmark examples. The synthesis algorithm is interactive and has several desirable features: it can rank multiple solutions and has fast convergence, it can detect noise in the user input, and it supports an active interaction model wherein the user is prompted to provide outputs on inputs that may have multiple computational interpretations. The algorithm has been implemented as an interactive add-in for Microsoft Excel spreadsheet system. The prototype tool has met the golden test - it has synthesized part of itself, and has been used to solve problems beyond author's imagination.},
  author = {Gulwani, Sumit},
  doi = {10.1145/1926385.1926423},
  isbn = {9781450304900},
  issn = {03621340},
  journal = {SIGPLAN Notices},
  keywords = {ample,pbe,program synthesis,programming ex,spreadsheet programming,user intent,version space algebra},
  number = {1},
  pages = {317--330},
  publisher = {ACM},
  series = {POPL '11},
  title = {{Automating String Processing in Spreadsheets Using Input-Output Examples}},
  url = {http://dl.acm.org/citation.cfm?id=1926423},
  volume = {46},
  year = {2011}
}

@inproceedings{Robertson2005,
  abstract = {Cerebral (Cell Region-Based Rendering And Layout) is an open-source Java plugin for the Cytoscape biomolecular interaction viewer. Given an interaction network and subcellular localization annotation, Cerebral automatically generates a view of the network in the style of traditional pathway diagrams, providing an intuitive interface for the exploration of a biological pathway or system. The molecules are separated into layers according to their subcellular localization. Potential products or outcomes of the pathway can be shown at the bottom of the view, clustered according to any molecular attribute data-protein function-for example. Cerebral scales well to networks containing thousands of nodes. AVAILABILITY: http://www.pathogenomics.ca/cerebral},
  author = {Robertson, George G and Czerwinski, Mary P and Churchill, John E},
  title = {{Visualization of Mappings Between Schemas}},
  booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
  series = {CHI '05},
  year = {2005},
  isbn = {1-58113-998-5},
  location = {Portland, Oregon, USA},
  pages = {431--439},
  numpages = {9},
  url = {http://doi.acm.org/10.1145/1054972.1055032},
  doi = {10.1145/1054972.1055032},
  acmid = {1055032},
  publisher = {ACM},
  address = {New York, NY, USA},
  keywords = {XML, XSLT, hierarchy visualization, interaction techniques, schema mapping, visualization}
}

@inproceedings{Haas2005,
  abstract = {Clio, the IBM Research system for expressing declarative schema mappings, has progressed in the past few years from a research prototype into a technology that is behind some of IBM's mapping technology. Clio provides a declarative way of specifying ...},
  author = {Haas, Laura M and Hern\'{a}ndez, Mauricio A and Ho, Howard and Popa, Lucian and Roth, Mary},
  title = {{Clio Grows Up: From Research Prototype to Industrial Tool}},
  booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
  series = {SIGMOD '05},
  year = {2005},
  isbn = {1-59593-060-4},
  location = {Baltimore, Maryland},
  pages = {805--810},
  numpages = {6},
  url = {http://doi.acm.org/10.1145/1066157.1066252},
  doi = {10.1145/1066157.1066252},
  acmid = {1066252},
  publisher = {ACM},
  address = {New York, NY, USA}
}

@inproceedings{Huynh2007,
  author = {David F Huynh, Robert C Miller, David R Karger},
  journal = {The Semantic Web},
  booktitle = {Proceedings of the 6th International Semantic Web Conference and 2nd Asian Semantic Web Conference},
  series = {ISWC + ASWC '07},
  pages = {903--910},
  publisher = {Springer},
  title = {{Potluck: Semi-ontology Alignment for Casual Users}},
  address = {Busan, Korea},
  year = {2007}
}

@article{Fisher2005,
  author = {Fisher, Kathleen and Gruber, Robert},
  title = {{PADS: A Domain-specific Language for Processing Ad Hoc Data}},
  journal = {SIGPLAN Notices},
  issue_date = {June 2005},
  volume = {40},
  number = {6},
  month = jun,
  year = {2005},
  issn = {0362-1340},
  pages = {295--304},
  numpages = {10},
  url = {http://doi.acm.org/10.1145/1064978.1065046},
  doi = {10.1145/1064978.1065046},
  acmid = {1065046},
  publisher = {ACM},
  address = {New York, NY, USA},
  keywords = {data description language, domain-specific languages},
}

@article{Fielding2000,
  abstract = {The World Wide Web has succeeded in large part because its software architecture has been designed to meet  the needs of an Internet-scale distributed hypermedia system. The Web has been iteratively developed over the past ten   years through a series of modifications to the standards that define its architecture. In order to identify those   aspects of the Web that needed improvement and avoid undesirable modifications, a model for the modern Web architecture   was needed to guide its design, definition, and deployment. Software architecture research investigates methods for   determining how best to partition a system, how components identify and communicate with each other, how information is   communicated, how elements of a system can evolve independently, and how all of the above can be described using formal   and informal notations. My work is motivated by the desire to understand and evaluate the architectural design of   network-based application software through principled use of architectural constraints, thereby obtaining the   functional, performance, and social properties desired of an architecture. An architectural style is a named,   coordinated set of architectural constraints. This dissertation defines a framework for understanding software  architecture via architectural styles and demonstrates how styles can be used to guide the architectural design of   network-based application software. A survey of architectural styles for network-based applications is used to classify   styles according to the architectural properties they induce on an architecture for distributed hypermedia. I then  introduce the Representational State Transfer (REST) architectural style and describe how REST has been used to guide  the design and development of the architecture for the modern Web. REST emphasizes scalability of component  interactions, generality of interfaces, independent deployment of components, and intermediary components to reduce  interaction latency, enforce security, and encapsulate legacy systems. I describe the software engineering principles  guiding REST and the interaction constraints chosen to retain those principles, contrasting them to the constraints of   other architectural styles. Finally, I describe the lessons learned from applying REST to the design of the Hypertext   Transfer Protocol and Uniform Resource Identifier standards, and from their subsequent deployment in Web client and   server software.},
  archivePrefix = {arXiv},
  arxivId = {arXiv:1011.1669v3},
  author = {Fielding, Roy T},
  doi = {10.1.1.91.2433},
  eprint = {arXiv:1011.1669v3},
  isbn = {0599871180},
  issn = {1098-6596},
  journal = {Building},
  pages = {162},
  pmid = {25246403},
  title = {{Architectural Styles and the Design of Network-based Software Architectures}},
  url = {http://www.ics.uci.edu/{~}fielding/pubs/dissertation/top.htm},
  volume = {54},
  year = {2000}
}

@article{Bostock2011,
  author = {Bostock, Michael and Ogievetsky, Vadim and Heer, Jeffrey},
  title = {{D3: Data-Driven Documents}},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  issue_date = {December 2011},
  volume = {17},
  number = {12},
  month = dec,
  year = {2011},
  issn = {1077-2626},
  pages = {2301--2309},
  url = {http://dx.doi.org/10.1109/TVCG.2011.185},
  doi = {10.1109/TVCG.2011.185},
  publisher = {IEEE Educational Activities Department},
  address = {Piscataway, NJ, USA},
}

@article{Forsell2010,
  abstract = {Evaluation is a key research challenge within the international Information Visualization (InfoVis) community, and Heuristic Evaluation is one recognized method. Various sets of heuristics have been proposed but there remains no consensus as to which heuristics are most useful for addressing aspects specific to the complex interactive visual displays used in modern InfoVis systems. This paper presents a first effort to empirically determine a new set of such general heuristics tailored for Heuristic Evaluation of common and important usability problems in InfoVis techniques. Participants in the study rated how well a total of 63 heuristics from 6 earlier published heuristic sets could explain a collection of 74 usability problems derived from earlier InfoVis evaluations. The results were used to synthesize 10 heuristics that, as a set, provided the highest explanatory coverage. The paper also stresses the challenges for future research to validate and further improve upon this set.},
  archivePrefix = {arXiv},
  arxivId = {arXiv:1011.1669v3},
  author = {Forsell, Camilla and Johansson, Jimmy},
  doi = {10.1145/1842993.1843029},
  eprint = {arXiv:1011.1669v3},
  isbn = {9781450300766},
  issn = {10603425},
  journal = {AVI '10 Proceedings of the International Conference on Advanced Visual Interfaces},
  keywords = {heuristic evaluation,heuristics,information visualization},
  number = {3},
  pages = {199--206},
  pmid = {44110946},
  title = {{An Heuristic Set for Evaluation in Information Visualization}},
  url = {http://dl.acm.org/citation.cfm?id=1843029{\%}5Cnhttp://www.mendeley.com/research/  heuristic-set-evaluation-information-visualization-1?utm{\_}source=desktop{\&}utm{\_}medium=1.5{\&}utm{\_}campaign=open{  \_}catalog{\&}userDocumentId={\%}7B10997496-0f1b-45ad-824b-6571c48b2b28{\%}7D},
  volume = {10},
  year = {2010}
}

@inproceedings{Chiticariu2008,
  abstract = {Schema integration is the problem of creating a unified target schema based on a set of existing source schemas that relate to each other via specified correspondences. The unified schema gives a standard representation of the data, thus offering a way to deal with the heterogeneity in the sources. In this paper, we develop a method and a design tool that provide: 1) adaptive enumeration of multiple interesting integrated schemas, and 2) easy-to-use capabilities for refining the enumerated schemas via user interaction. Our method is a departure from previous approaches to schema integration, which do not offer a systematic exploration of the possible integrated schemas. The method operates at a logical level, where we recast each source schema into a graph of concepts with Has-A relationships. We then identify matching concepts in different graphs by taking into account the correspondences between their attributes. For every pair of matching concepts, we have two choices: merge them into one integrated concept or keep them as separate concepts. We develop an algorithm that can systematically output, without duplication, all possible integrated schemas resulting from the previous choices. For each integrated schema, the algorithm also generates a mapping from the source schemas to the integrated schema that has precise information-preserving properties. Furthermore, we avoid a full enumeration, by allowing users to specify constraints on the merging process, based on the schemas produced so far. These constraints are then incorporated in the enumeration of the subsequent schemas. The result is an adaptive and interactive enumeration method that significantly reduces the space of alternative schemas, and facilitates the selection of the final integrated schema.},
  author = {Chiticariu, Laura and Kolaitis, Phokion G and Popa, Lucian},
  doi = {10.1145/1376616.1376700},
  editor = {Wang, Jason Tsong-Li},
  isbn = {9781605581026},
  booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
  keywords = {concept graph,data,interactive generation,model management,schema integration,schema mapping,tegration},
  pages = {833},
  publisher = {ACM},
  series = {SIGMOD '08},
  title = {{Interactive Generation of Integrated Schemas}},
  url = {http://portal.acm.org/citation.cfm?doid=1376616.1376700},
  address = {Vancouver, BC, Canada},
  year = {2008}
}

@inproceedings{Dasu2002,
  author = {Dasu, Tamraparni and Johnson, Theodore and Muthukrishnan, S and Shkapenyuk, Vladislav},
  booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
  doi = {10.1145/564691.564719},
  isbn = {1-58113-497-5},
  pages = {240--251},
  publisher = {ACM},
  series = {SIGMOD '02},
  title = {{Mining Database Structure; Or, How to Build a Data Quality Browser}},
  url = {http://doi.acm.org/10.1145/564691.564719},
  address = {Madison, WI, USA},
  year = {2002}
}

@inproceedings{Bernard2012,
  author    = {J{\"u}rgen Bernard and
               Tobias Ruppert and
               Oliver Goroll and
               Thorsten May and
               J{\"o}rn Kohlhammer},
  title     = {{Visual-interactive Preprocessing of Time Series Data}},
  booktitle = {SIGRAD},
  year      = {2012},
  pages     = {39-48},
  ee        = {http://www.ep.liu.se/ecp_article/index.en.aspx?issue=081;article=006},
  crossref  = {DBLP:conf/sigrad/2012},
  bibsource = {DBLP, http://dblp.uni-trier.de}
}

@proceedings{DBLP:conf/sigrad/2012,
  editor    = {Andreas Kerren and
               Stefan Seipel},
  title     = {Proceedings of SIGRAD 2012, Interactive Visual Analysis
               of Data, V{\"a}xj{\"o}, Sweden, November 29-30, 2012},
  booktitle = {SIGRAD},
  publisher = {Link{\"o}ping University Electronic Press},
  series    = {Link{\"o}ping Electronic Conference Proceedings},
  volume    = {81},
  year      = {2012},
  isbn      = {978-91-7519-723-4},
  bibsource = {DBLP, http://dblp.uni-trier.de}
}

@misc{Hellerstein2008,
  author = {Hellerstein, Joseph M},
  howpublished = {United Nations Economic Commission for Europe (UNECE)},
  keywords = {cidr2011,tkde,uist2010},
  title = {{Quantitative Data Cleaning for Large Databases}},
  year = {2008}
}

@misc{web:DataScience,
  author = {{Mike Loukides}},
  title = {{What is Data Science?}},
  month = Jun,
  year = {2010},
  howpublished = {O'Reilly Media},
  note = {https://www.oreilly.com/ideas/what-is-data-science, Accessed: 2017-04-11}
}

@misc{web:Timelion,
  author = {{Rashid Khan}},
  title = {{Timelion: The Time Series Composer for Kibana}},
  month = Nov,
  year = {2015},
  howpublished = {Elastic},
  note = {https://www.elastic.co/blog/timelion-timeline, Accessed: 2017-04-11}
}
